{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc565ac7-a7cb-43aa-8e7b-7cd17901667f",
   "metadata": {},
   "source": [
    "# Exercise 7 / Subgradient Method\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Implement a smooth approximation of:\n",
    "    - ReLU\n",
    "    - absolute value\n",
    "\n",
    "1. Develop Subgradient method\n",
    "\n",
    "1. Compute subgradient of:\n",
    "    - robust regression\n",
    "    - linear regression with $\\mathcal{l}_1$ regularization\n",
    "    - logistic regression with $\\mathcal{l}_1$ regularization\n",
    "\n",
    "### Submission\n",
    "\n",
    "When done, paste your code into the quiz on Moodle and answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31b735-7a43-48e7-895f-6ac3467931e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ab7ff-2090-4470-9bf5-6eff31c862a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from typing import Callable\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526ec1b-fb2c-4e29-8c47-a07b6609fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_map(\n",
    "    f: Callable[[NDArray], float],\n",
    "    xb: tuple[float, float] = (-1.0, 1.0),\n",
    "    yb: tuple[float, float] = (-1.0, 1.0),\n",
    "    ax=None,\n",
    ") -> None:\n",
    "    \"\"\"Plots the contour lines of a scalar function on a 2D grid.\n",
    "\n",
    "    Args:\n",
    "        f (Callable[[NDArray], float]): Scalar function mapping points to values.\n",
    "        xb (tuple[float, float], optional): Lower and upper bounds for the x-axis. Defaults to (-1.0, 1.0).\n",
    "        yb (tuple[float, float], optional): Lower and upper bounds for the y-axis. Defaults to (-1.0, 1.0).\n",
    "        ax (optional): The axes to plot on. Defaults to None.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt\n",
    "    (nx, ny) = (45, 45)\n",
    "    x = np.linspace(*xb, nx)\n",
    "    y = np.linspace(*yb, ny)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    grid = np.block([[xv.reshape(1, -1)], [yv.reshape(1, -1)]]).T\n",
    "    values = np.fromiter((f(point) for point in grid), dtype=np.double)\n",
    "    ax.contour(xv, yv, values.reshape(nx, ny), 15)\n",
    "\n",
    "\n",
    "def surface_plot(\n",
    "    f: Callable[[NDArray], float],\n",
    "    xb: tuple[float, float] = (-1.0, 1.0),\n",
    "    yb: tuple[float, float] = (-1.0, 1.0),\n",
    ") -> tuple[plt.Figure, Axes3D]:\n",
    "    \"\"\"Creates a 3D surface plot of a scalar function on a 2D grid.\n",
    "\n",
    "    Args:\n",
    "        f (Callable[[NDArray], float]): Scalar function mapping points to values.\n",
    "        xb (tuple[float, float], optional): Lower and upper bounds for the x-axis. Defaults to (-1.0, 1.0).\n",
    "        yb (tuple[float, float], optional): Lower and upper bounds for the y-axis. Defaults to (-1.0, 1.0).\n",
    "    Returns:\n",
    "        tuple[plt.Figure, Axes3D]: Figure and axis containing the rendered surface.\n",
    "    \"\"\"\n",
    "    (nx, ny) = (45, 45)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    x = np.linspace(*xb, nx)\n",
    "    y = np.linspace(*yb, ny)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    grid = np.block([[xv.reshape(1, -1)], [yv.reshape(1, -1)]]).T\n",
    "    values = np.fromiter((f(point) for point in grid), dtype=np.double)\n",
    "    ax.plot_surface(xv, yv, values.reshape(nx, ny), cmap=cm.coolwarm)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES = [\n",
    "    (\n",
    "        lambda x: abs(x[0]) + abs(8 * x[1]),\n",
    "        lambda x: np.sign(x) * np.array([1, 8]),\n",
    "        np.array([-0.5, 0.5]),\n",
    "        (-1.0, 1.0),\n",
    "        (-1.0, 1.0),\n",
    "    ),\n",
    "    (\n",
    "        lambda x: abs(3 * x[0]) + abs(0.5 * x[1]),\n",
    "        lambda x: np.sign(x) * np.array([3, 0.5]),\n",
    "        np.array([0.5, -0.5]),\n",
    "        (-1.0, 1.0),\n",
    "        (-1.0, 1.0),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def run_examples(\n",
    "    optimizer: Callable,\n",
    "    optimizer_name: str,\n",
    ") -> None:\n",
    "    \"\"\"Runs the subgradient method on example functions and visualizes the path.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Callable): The optimizer to test.\n",
    "        optimizer_name (str): Name of the optimizer.\n",
    "    \"\"\"\n",
    "    for f, g, x0, x_bounds, y_bounds in EXAMPLES:\n",
    "        xs = np.array(optimizer(x0, f, g))\n",
    "\n",
    "        contour_map(f, xb=x_bounds, yb=y_bounds)\n",
    "        plt.plot(xs[:, 0], xs[:, 1], \".--k\", label=optimizer_name)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(\n",
    "    x0: NDArray,\n",
    "    f: Callable,\n",
    "    g: Callable,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0 (NDArray): The starting point.\n",
    "        f (Callable): The objective function.\n",
    "        g (Callable]): The gradient of the objective function.\n",
    "    \"\"\"\n",
    "    subgrad = subgradient_path(x0, f, g)\n",
    "\n",
    "    f_values = np.array([f(x) for x in subgrad])\n",
    "\n",
    "    plt.semilogy(\n",
    "        np.arange(len(subgrad)),\n",
    "        f_values,\n",
    "        label=\"subgradient\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Function value\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smooth_approximation(\n",
    "    original_function: Callable,\n",
    "    smoothed_function: Callable,\n",
    "    M_values: NDArray,\n",
    "    x_bounds: tuple[float, float] = (-0.25, 0.25),\n",
    "    n_points: int = 1000,\n",
    "    function_name: str = \"f\",\n",
    ") -> None:\n",
    "    \"\"\"Plots a function and its smooth approximations for different M values.\n",
    "\n",
    "    Args:\n",
    "        f (Callable): The original function to approximate.\n",
    "        g (Callable): The smooth approximation function.\n",
    "        M_values (NDArray): Array of M values to use for smoothing parameter.\n",
    "        x_bounds (tuple[float, float], optional): Lower and upper bounds for x-axis. Defaults to (-0.25, 0.25).\n",
    "        n_points (int, optional): Number of points to plot. Defaults to 1000.\n",
    "        function_name (str, optional): Name of the function for the legend. Defaults to \"f\".\n",
    "    \"\"\"\n",
    "\n",
    "    xx = np.linspace(*x_bounds, n_points)\n",
    "\n",
    "    plt.plot(xx, original_function(xx), linewidth=2, label=function_name)\n",
    "    for M in M_values:\n",
    "        plt.plot(xx, smoothed_function(xx, M), label=f\"M={M:.3f}\")\n",
    "\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Function value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3173a-453d-48fa-b0c0-8fddb861481c",
   "metadata": {},
   "source": [
    "## Task 1: Smoothing plots\n",
    "\n",
    "Using the idea from **Task 4** from the **Theory** part, plot the \"smooth\" approximation of the following functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db7ebc-8bcd-42f5-9201-5dafc603e962",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "Plot an approximation from Task 4 for the function\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(x, 0)\n",
    "$$\n",
    "for different values of $M$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6a515-7830-4816-8a43-c216f8b4294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.logspace(1, 2, 5)\n",
    "\n",
    "\n",
    "# ReLU\n",
    "def f(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "# Smooth approximation of ReLU\n",
    "def g(x, M):\n",
    "    # TODO: implement smooth approximation of ReLU\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "plot_smooth_approximation(f, g, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3db80-4175-42ae-bac9-0f7e7ec4baaf",
   "metadata": {},
   "source": [
    "### Absolute value\n",
    "\n",
    "Plot an approximation from Task 4 for the function\n",
    "$$\n",
    "\\text{abs}(x) = |x|\n",
    "$$\n",
    "for different values of $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03064853-958c-4b08-bd7b-40e90fc93b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.logspace(1, 2, 5)\n",
    "\n",
    "\n",
    "# Absolute value\n",
    "def f(x):\n",
    "    return np.abs(x)\n",
    "\n",
    "\n",
    "# Smooth approximation of absolute value\n",
    "def g(x, M):\n",
    "    # TODO: implement smooth approximation of absolute value\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "plot_smooth_approximation(f, g, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e5872-402c-45c2-b9f0-08aa2cbcb718",
   "metadata": {},
   "source": [
    "## Task 2: Sub-gradient method\n",
    "\n",
    "Implement the sub-gradient method.\n",
    "* `x0` is the initial point.\n",
    "* `f` is the function you are trying to minimize.\n",
    "* `g` is the subgradient of `f`.\n",
    "\n",
    "Function `subgradient_path` should return a list of vectors on the path to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826621a3-fb82-4d1c-949f-509d2a7abc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_path(\n",
    "    x0: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    max_iter: int = 1000,\n",
    ") -> list[NDArray]:\n",
    "    \"\"\"Performs the subgradient method with diminishing step size.\n",
    "\n",
    "    Args:\n",
    "        x0 (NDArray): The starting point.\n",
    "        f (Callable[[NDArray], float]): The objective function.\n",
    "        g (Callable[[NDArray], NDArray]): The subgradient of the objective function.\n",
    "        max_iter (int, optional): The maximum number of steps. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        list[NDArray]: The list of points visited during the optimization.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the subgradient method with diminishing step size.\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91490c83-aeee-4192-84b5-9e2a92c0f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to get plots\n",
    "run_examples(\n",
    "    optimizer=subgradient_path,\n",
    "    optimizer_name=\"subgradient\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa600c6b",
   "metadata": {},
   "source": [
    "## Task 3: Error plots\n",
    "\n",
    "Then compare and plot the error over time for the five methods on the following tasks.\n",
    "Implementations of the prior methods are given in the Utils section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d4fb4-bc68-4edd-b877-13eefe7f52c9",
   "metadata": {},
   "source": [
    "### Robust regresssion\n",
    "\n",
    "Finish the gradient of the function\n",
    "$$\n",
    "f(w) = \\frac{1}{n}\\left\\|Xw - y\\right\\|_1\n",
    "$$\n",
    "run the sub-gradient method on it, and plot the function over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a3354-d156-45e2-b09e-507ccbe5e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    return np.linalg.norm(X @ w - y, 1) / len(X)\n",
    "\n",
    "\n",
    "def g(w):\n",
    "    # TODO: Implement the subgradient\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=40, random_state=0)\n",
    "x0 = np.zeros(100)\n",
    "\n",
    "plot_function(x0, f, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510aa632-0b89-449f-a81b-5d38eec3e2be",
   "metadata": {},
   "source": [
    "### Linear regression with $\\mathcal{l}_1$ regularization\n",
    "\n",
    "Finish the gradient of the function\n",
    "$$\n",
    "f(w) = \\frac{1}{n}\\left\\|Xw - y\\right\\|_2^2 + \\|w\\|_1,\n",
    "$$\n",
    "run the sub-gradient method on it, and plot the function over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9e1b6-a8e4-42f8-80c2-2661b099f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    return np.linalg.norm(X @ w - y) ** 2 / len(X) + np.linalg.norm(w, 1)\n",
    "\n",
    "\n",
    "def g(w):\n",
    "    # TODO: Implement the subgradient\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=40, random_state=0)\n",
    "x0 = np.zeros(100)\n",
    "\n",
    "plot_function(x0, f, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104abd0-5728-4b25-ada7-3c574379f339",
   "metadata": {},
   "source": [
    "### Logistic regression with $\\mathcal{l}_1$ regularization\n",
    "\n",
    "Finish the gradient of the function\n",
    "$$\n",
    "f(w) = \\frac{1}{n}\\sum_{i=1}^n \\log(1 + \\exp(-y_i \\cdot x_i^\\top w)) + \\|w\\|_1\n",
    "$$\n",
    "run the sub-gradient method on it, and plot the function over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacc5fb-e053-4704-9634-ffe60ee42186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    return np.log(1.0 + np.exp(-y * X.dot(w))).mean() + np.linalg.norm(w, 1)\n",
    "\n",
    "\n",
    "def g(w):\n",
    "    # TODO: Implement the subgradient\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "    1000,\n",
    "    80,\n",
    "    n_informative=40,\n",
    "    # n_redundant=0,\n",
    "    n_clusters_per_class=2,\n",
    "    flip_y=0.1,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "x0 = np.zeros(80)\n",
    "\n",
    "plot_function(x0, f, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b40d6-5f92-424e-8cb7-cf44fab42b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
