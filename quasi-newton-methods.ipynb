{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc565ac7-a7cb-43aa-8e7b-7cd17901667f",
   "metadata": {},
   "source": [
    "# Exercise 6 / Quasi-Newton Methods\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Implement DFP method and compare it with first-order methods\n",
    "\n",
    "1. Implement BFGS method and compare it with first-order methods\n",
    "\n",
    "1. Analyze error plots of the respective optimization methods applied for different tasks\n",
    "\n",
    "### Submission\n",
    "\n",
    "When done, paste your code into the quiz on Moodle and answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31b735-7a43-48e7-895f-6ac3467931e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3ab7ff-2090-4470-9bf5-6eff31c862a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from typing import Callable\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0526ec1b-fb2c-4e29-8c47-a07b6609fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_map(\n",
    "    f: Callable[[NDArray], float],\n",
    "    xb: tuple[float, float] = (-1.0, 1.0),\n",
    "    yb: tuple[float, float] = (-1.0, 1.0),\n",
    "    ax=None,\n",
    ") -> None:\n",
    "    \"\"\"Plots the contour lines of a scalar function on a 2D grid.\n",
    "\n",
    "    Args:\n",
    "        f (Callable[[NDArray], float]): Scalar function mapping points to values.\n",
    "        xb (tuple[float, float], optional): Lower and upper bounds for the x-axis. Defaults to (-1.0, 1.0).\n",
    "        yb (tuple[float, float], optional): Lower and upper bounds for the y-axis. Defaults to (-1.0, 1.0).\n",
    "        ax (optional): The axes to plot on. Defaults to None.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt\n",
    "    (nx, ny) = (45, 45)\n",
    "    x = np.linspace(*xb, nx)\n",
    "    y = np.linspace(*yb, ny)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    grid = np.block([[xv.reshape(1, -1)], [yv.reshape(1, -1)]]).T\n",
    "    values = np.fromiter((f(point) for point in grid), dtype=np.double)\n",
    "    ax.contour(xv, yv, values.reshape(nx, ny), 15)\n",
    "\n",
    "\n",
    "def surface_plot(\n",
    "    f: Callable[[NDArray], float],\n",
    "    xb: tuple[float, float] = (-1.0, 1.0),\n",
    "    yb: tuple[float, float] = (-1.0, 1.0),\n",
    ") -> tuple[plt.Figure, Axes3D]:\n",
    "    \"\"\"Creates a 3D surface plot of a scalar function on a 2D grid.\n",
    "\n",
    "    Args:\n",
    "        f (Callable[[NDArray], float]): Scalar function mapping points to values.\n",
    "        xb (tuple[float, float], optional): Lower and upper bounds for the x-axis. Defaults to (-1.0, 1.0).\n",
    "        yb (tuple[float, float], optional): Lower and upper bounds for the y-axis. Defaults to (-1.0, 1.0).\n",
    "    Returns:\n",
    "        tuple[plt.Figure, Axes3D]: Figure and axis containing the rendered surface.\n",
    "    \"\"\"\n",
    "    (nx, ny) = (45, 45)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    x = np.linspace(*xb, nx)\n",
    "    y = np.linspace(*yb, ny)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    grid = np.block([[xv.reshape(1, -1)], [yv.reshape(1, -1)]]).T\n",
    "    values = np.fromiter((f(point) for point in grid), dtype=np.double)\n",
    "    ax.plot_surface(xv, yv, values.reshape(nx, ny), cmap=cm.coolwarm)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4139b34-74b0-4c7b-8094-9fe78ede4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(\n",
    "    x: NDArray,\n",
    "    d: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    alpha: float = 0.3,\n",
    "    beta: float = 0.8,\n",
    ") -> float:\n",
    "    \"\"\"Performs Armijo backtracking line search for a descent direction.\n",
    "\n",
    "    Args:\n",
    "        x (NDArray): Current point in parameter space.\n",
    "        d (NDArray): Candidate descent direction.\n",
    "        f (Callable[[NDArray], float]): Objective function producing scalar values.\n",
    "        g (Callable[[NDArray], NDArray]): Gradient of the objective function.\n",
    "        alpha (float, optional): Armijo sufficient decrease parameter. Defaults to 0.3.\n",
    "        beta (float, optional): Multiplicative shrink factor for the step size. Defaults to 0.8.\n",
    "    Returns:\n",
    "        float: Step length that satisfies the Armijo condition.\n",
    "    \"\"\"\n",
    "    step_size = 1.0\n",
    "    fx = f(x)\n",
    "    grad_x = g(x)\n",
    "    directional_derivative = grad_x.dot(d)\n",
    "\n",
    "    while f(x + step_size * d) > fx + alpha * step_size * directional_derivative:\n",
    "        prev_step_size = step_size\n",
    "        step_size *= beta\n",
    "        if prev_step_size == step_size: break # If the floating point numbers are equal, we have reached the machine precision limits.\n",
    "\n",
    "    return step_size\n",
    "\n",
    "\n",
    "def non_descent_backtracking_line_search(\n",
    "    x: NDArray,\n",
    "    d: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 0.8,\n",
    ") -> float:\n",
    "    \"\"\"Line search for methods that do not guarantee a descent direction.\n",
    "    Finds a step size that satisfies a modified sufficient decrease condition.\n",
    "\n",
    "    Args:\n",
    "        x (NDArray): The current point.\n",
    "        d (NDArray): The search direction (not necessarily a descent direction).\n",
    "        f (Callable[[NDArray], float]): The objective function.\n",
    "        g (Callable[[NDArray], NDArray]): The gradient of the objective function.\n",
    "        alpha (float, optional): The parameter of the modified sufficient decrease condition. Defaults to 0.3.\n",
    "        beta (float, optional): The factor by which to reduce the step size per iteration. Defaults to 0.8.\n",
    "\n",
    "    Returns:\n",
    "        float: The step size that satisfies the modified sufficient decrease condition.\n",
    "    \"\"\"\n",
    "    step_size = 1.0\n",
    "    fx: float = f(x)\n",
    "    grad_x = g(x)\n",
    "    directional_derivative = grad_x.dot(d)\n",
    "    while (\n",
    "        f(x + step_size * d)\n",
    "        > fx + alpha * step_size * directional_derivative + step_size / 2 * d.T @ d\n",
    "    ):\n",
    "        prev_step_size = step_size\n",
    "        step_size *= beta\n",
    "        if prev_step_size == step_size: break # If the floating point numbers are equal, we have reached the machine precision limits.\n",
    "\n",
    "    return step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3f0355c-8e11-435a-b4f6-5c4253fd95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_path(\n",
    "    x0: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    max_iter: int = 100,\n",
    ") -> list[NDArray]:\n",
    "    \"\"\"Collects iterates produced by gradient descent.\n",
    "\n",
    "    Args:\n",
    "        x0 (NDArray): Initial point used to seed the algorithm.\n",
    "        f (Callable[[NDArray], float]): Objective function producing scalar values.\n",
    "        g (Callable[[NDArray], NDArray]): Gradient of the objective function.\n",
    "        max_iter (int, optional): Maximum number of gradient steps. Defaults to 100.\n",
    "    Returns:\n",
    "        list[NDArray]: Sequence of iterates, including the initial point.\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float, copy=True)\n",
    "    xs = [x.copy()]\n",
    "    for _ in range(max_iter):\n",
    "        grad = g(x)\n",
    "        step = backtracking_line_search(x, -grad, f, g)\n",
    "        x = x - step * grad\n",
    "        xs.append(x.copy())\n",
    "    return xs\n",
    "\n",
    "\n",
    "def heavy_ball_path(\n",
    "    x0: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    max_iter: int = 100,\n",
    ") -> list[NDArray]:\n",
    "    \"\"\"Performs the Heavy Ball method.\n",
    "\n",
    "    Args:\n",
    "        x0 (NDArray): The starting point.\n",
    "        f (Callable[[NDArray], float]): The objective function\n",
    "        g (Callable[[NDArray], NDArray]): The gradient of the objective function.\n",
    "        max_iter (int, optional): The maximum number of steps. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        list[NDArray]: The list of points visited during the optimization.\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float, copy=True)\n",
    "    xs = [x]\n",
    "    for i in range(1, max_iter + 1):\n",
    "        current_x = xs[-1]\n",
    "        prev_x = xs[-2] if len(xs) > 1 else xs[-1]\n",
    "        d = -g(current_x) + (i - 1) / (i + 1) * (current_x - prev_x)\n",
    "        t = non_descent_backtracking_line_search(current_x, d, f, g)\n",
    "        xs.append(current_x + t * d)\n",
    "\n",
    "    return xs\n",
    "\n",
    "\n",
    "def nag_path(\n",
    "    x0: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    max_iter: int = 100,\n",
    ") -> list[NDArray]:\n",
    "    \"\"\"Performs Nesterov's accelerated gradient method.\n",
    "\n",
    "    Args:\n",
    "        x0 (NDArray): The starting point.\n",
    "        f (Callable[[NDArray], float]): The objective function\n",
    "        g (Callable[[NDArray], NDArray]): The gradient of the objective function.\n",
    "        max_iter (int, optional): The maximum number of steps. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        list[NDArray]: The list of points visited during the optimization.\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float, copy=True)\n",
    "    xs = [x]\n",
    "    for i in range(1, max_iter + 1, 1):\n",
    "        current_x = xs[-1]\n",
    "        prev_x = xs[-2] if len(xs) > 1 else xs[-1]\n",
    "        y = current_x + (i - 1) / (i + 2) * (current_x - prev_x)\n",
    "        d = g(y)\n",
    "        t = non_descent_backtracking_line_search(y, -d, f, g)\n",
    "        xs.append(y - t * d)\n",
    "\n",
    "    return xs\n",
    "\n",
    "\n",
    "def newtons_path(\n",
    "    x0: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    h: Callable[[NDArray], NDArray],\n",
    "    max_iter: int = 100,\n",
    ") -> list[NDArray]:\n",
    "    \"\"\"Performs Newton's method.\n",
    "\n",
    "    Args:\n",
    "        x0 (NDArray): The starting point.\n",
    "        f (Callable[[NDArray], float]): The objective function\n",
    "        g (Callable[[NDArray], NDArray]): The gradient of the objective function.\n",
    "        h (Callable[[NDArray], NDArray]): The second gradient of the objective function.\n",
    "        max_iter (int, optional): The maximum number of steps. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        list[NDArray]: The list of points visited during the optimization.\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float, copy=True)\n",
    "    xs = [x.copy()]\n",
    "    for _ in range(max_iter):\n",
    "        d = np.linalg.solve(h(x), -g(x))\n",
    "        t = backtracking_line_search(x, d, f, g)\n",
    "        x = x + t * d\n",
    "        xs.append(x.copy())\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5816f59c-9dba-4f72-a144-271e5152046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_MATRIX = np.array([[30.0, 15], [-20, 25]]) / 20\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\n",
    "        lambda x: x.T @ EXAMPLE_MATRIX @ x + 1,\n",
    "        lambda x: (EXAMPLE_MATRIX + EXAMPLE_MATRIX.T) @ x,\n",
    "        lambda x: (EXAMPLE_MATRIX + EXAMPLE_MATRIX.T),\n",
    "        0.5 * np.ones(2),\n",
    "        (-1.0, 1.0),\n",
    "        (-1.0, 1.0),\n",
    "    ),\n",
    "    (\n",
    "        lambda x: (x[0] ** 2 + 30 * x[1] ** 2 + 4 * x[0]),\n",
    "        lambda x: np.array([2 * x[0] + 4, 60 * x[1]]),\n",
    "        lambda x: np.array([[2, 0], [0, 60]]),\n",
    "        np.array([2.0, 3.0]),\n",
    "        (-5, 5),\n",
    "        (-4, 4),\n",
    "    ),\n",
    "    (\n",
    "        lambda x: np.linalg.norm(np.sin(x * 3)) ** 2,\n",
    "        lambda x: 6 * np.sin(x * 3) * np.cos(x * 3),\n",
    "        lambda x: 18 * np.diag(2 * np.cos(3 * x) ** 2 - 1),\n",
    "        np.array([0.2, 0.15]),\n",
    "        (-0.5, 0.5),\n",
    "        (-0.5, 0.5),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def run_examples(\n",
    "    optimizer_1: Callable,\n",
    "    optimizer_name_1: str,\n",
    "    optimizer_2: Callable,\n",
    "    optimizer_name_2: str,\n",
    "    uses_hessian_1: bool = False,\n",
    "    uses_hessian_2: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Compares one optimizer to another one.\n",
    "\n",
    "    Args:\n",
    "        optimizer_1 (Callable): The optimizer to test.\n",
    "        optimizer_name_1 (str): Name of the first optimizer.\n",
    "        optimizer_2: optimizer_path_function_type): The second optimizer to compare against.\n",
    "        optimizer_name_2 (str): Name of the second optimizer.\n",
    "        uses_hessian_1 (bool, optional): Whether the first optimizer uses the Hessian function. Defaults to False.\n",
    "        uses_hessian_2 (bool, optional): Whether the second optimizer uses the Hessian function. Defaults to False.\n",
    "    \"\"\"\n",
    "    for f, g, h, x0, x_bounds, y_bounds in EXAMPLES:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "        xs_1 = np.array(\n",
    "            optimizer_1(x0, f, g, h) if uses_hessian_1 else optimizer_1(x0, f, g)\n",
    "        )\n",
    "        xs_2 = np.array(\n",
    "            optimizer_2(x0, f, g, h) if uses_hessian_2 else optimizer_2(x0, f, g)\n",
    "        )\n",
    "\n",
    "        contour_map(f, xb=x_bounds, yb=y_bounds, ax=axs[0])\n",
    "\n",
    "        axs[0].plot(xs_1[:, 0], xs_1[:, 1], \".--k\", label=optimizer_name_1)\n",
    "        axs[0].plot(\n",
    "            xs_2[:, 0],\n",
    "            xs_2[:, 1],\n",
    "            \".--\",\n",
    "            color=\"gray\",\n",
    "            alpha=0.5,\n",
    "            label=optimizer_name_2,\n",
    "        )\n",
    "\n",
    "        axs[1].semilogy(\n",
    "            np.arange(0, len(xs_1)), [f(x) for x in xs_1], label=optimizer_name_1\n",
    "        )\n",
    "        axs[1].semilogy(\n",
    "            np.arange(0, len(xs_2)), [f(x) for x in xs_2], label=optimizer_name_2\n",
    "        )\n",
    "\n",
    "        axs[0].legend()\n",
    "        axs[1].legend()\n",
    "        axs[1].set_xlabel(\"iteration\")\n",
    "        axs[1].set_ylabel(\"f(x_t)\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3173a-453d-48fa-b0c0-8fddb861481c",
   "metadata": {},
   "source": [
    "## Task 1: DFP method\n",
    "\n",
    "Implement the DFP method using the above provided backtracking line search.\n",
    "* `x0` is the initial point.\n",
    "* `f` is the function you are trying to minimize.\n",
    "* `g` is the gradient of `f`.\n",
    "\n",
    "Function `dfp_path` should return a list of vectors on the path to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfe6a515-7830-4816-8a43-c216f8b4294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfp_path(\n",
    "    x0: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    max_iter: int = 100,\n",
    ") -> list[NDArray]:\n",
    "    \"\"\"Performs the DFP (Davidon-Fletcher-Powell) quasi-Newton method.\n",
    "\n",
    "    Args:\n",
    "        x0 (NDArray): The starting point.\n",
    "        f (Callable[[NDArray], float]): The objective function.\n",
    "        g (Callable[[NDArray], NDArray]): The gradient of the objective function.\n",
    "        max_iter (int, optional): The maximum number of steps. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        list[NDArray]: The list of points visited during the optimization.\n",
    "    \"\"\"\n",
    "    # TODO: Implement DFP quasi-Newton method using the provided backtracking line search\n",
    "\n",
    "    # raise NotImplementedError\n",
    "    # eps = 0.000000001\n",
    "    # x = np.array(x0, dtype=float, copy=True)\n",
    "    xs = [x0.copy()]\n",
    "    H = np.eye(x0.shape[0])\n",
    "    Hs = [H] # first Hessian as identity; H=inv(B)\n",
    "    for i in range(max_iter):\n",
    "        prev_x = xs[-2] if len(xs) > 1 else xs[-1]\n",
    "        y = g(x0) - g(prev_x)\n",
    "        # y *= 3\n",
    "        # y[-1] -= 1\n",
    "        s = x0 - prev_x + 1\n",
    "        # s *= 2\n",
    "        print('s', s)\n",
    "        print('y', y)\n",
    "        print('H', H)\n",
    "        print('s', s.shape)\n",
    "        print('y', y.shape)\n",
    "        print('H', H.shape)\n",
    "        print()\n",
    "        # rho = 1 / y.T.dot(s)\n",
    "        num1 = (H @ y)\n",
    "        print(num1)\n",
    "        # num1 = np.outer(num1, y.T) @ H.T\n",
    "        num1 = np.outer(num1, y.T)\n",
    "        print(num1)\n",
    "        num1 = num1 @ H.T\n",
    "        # num1 = (H @ y @ y.T @ H.T)\n",
    "        print('num1', num1.shape)\n",
    "        print(num1)\n",
    "        print()\n",
    "        denom1 = (y.T @ H)\n",
    "        print(denom1)\n",
    "        denom1 = denom1 @ y\n",
    "        print('denom1', denom1.shape)\n",
    "        print(denom1)\n",
    "        print()\n",
    "        num2 = np.outer(s, s.T)\n",
    "        print(num2)\n",
    "        print('num2', num2.shape)\n",
    "        print()\n",
    "        denom2 = (y.T @ s)\n",
    "        print('denom2')\n",
    "        # H -= (H @ y @ y.T @ H.T) / (y.T @ H @ y) + (s @ s.T) / (y.T @ s)\n",
    "        H -= np.outer(H @ y, y.T) @ H.T / (y.T @ H @ y) + np.outer(s, s.T) / (y.T @ s)\n",
    "        # H -= num1 / denom1 + num2 / denom2\n",
    "        print()\n",
    "        print(H)\n",
    "        d = H\n",
    "        bls = backtracking_line_search(x0, -g(x0), f, g)\n",
    "        x0 += bls * d\n",
    "        xs.append(x0.copy())\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87802424-9573-4cea-9db8-1a3317ab5ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s [1. 1.]\n",
      "y [0. 0.]\n",
      "H [[1. 0.]\n",
      " [0. 1.]]\n",
      "s (2,)\n",
      "y (2,)\n",
      "H (2, 2)\n",
      "\n",
      "[0. 0.]\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "num1 (2, 2)\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "[0. 0.]\n",
      "denom1 ()\n",
      "0.0\n",
      "\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "num2 (2, 2)\n",
      "\n",
      "denom2\n",
      "\n",
      "[[nan nan]\n",
      " [nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14596\\1050335011.py:64: RuntimeWarning: invalid value encountered in divide\n",
      "  H -= np.outer(H @ y, y.T) @ H.T / (y.T @ H @ y) + np.outer(s, s.T) / (y.T @ s)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14596\\1050335011.py:64: RuntimeWarning: divide by zero encountered in divide\n",
      "  H -= np.outer(H @ y, y.T) @ H.T / (y.T @ H @ y) + np.outer(s, s.T) / (y.T @ s)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (2,) doesn't match the broadcast shape (2,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# run this to get plots\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdfp_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_name_1\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdfp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_descent_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_name_2\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgradient descent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mrun_examples\u001b[39m\u001b[34m(optimizer_1, optimizer_name_1, optimizer_2, optimizer_name_2, uses_hessian_1, uses_hessian_2)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f, g, h, x0, x_bounds, y_bounds \u001b[38;5;129;01min\u001b[39;00m EXAMPLES:\n\u001b[32m     50\u001b[39m     fig, axs = plt.subplots(nrows=\u001b[32m1\u001b[39m, ncols=\u001b[32m2\u001b[39m, figsize=(\u001b[32m12\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m     52\u001b[39m     xs_1 = np.array(\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         optimizer_1(x0, f, g, h) \u001b[38;5;28;01mif\u001b[39;00m uses_hessian_1 \u001b[38;5;28;01melse\u001b[39;00m \u001b[43moptimizer_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     )\n\u001b[32m     55\u001b[39m     xs_2 = np.array(\n\u001b[32m     56\u001b[39m         optimizer_2(x0, f, g, h) \u001b[38;5;28;01mif\u001b[39;00m uses_hessian_2 \u001b[38;5;28;01melse\u001b[39;00m optimizer_2(x0, f, g)\n\u001b[32m     57\u001b[39m     )\n\u001b[32m     59\u001b[39m     contour_map(f, xb=x_bounds, yb=y_bounds, ax=axs[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mdfp_path\u001b[39m\u001b[34m(x0, f, g, max_iter)\u001b[39m\n\u001b[32m     68\u001b[39m     d = H\n\u001b[32m     69\u001b[39m     bls = backtracking_line_search(x0, -g(x0), f, g)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbls\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\n\u001b[32m     71\u001b[39m     xs.append(x0.copy())\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xs\n",
      "\u001b[31mValueError\u001b[39m: non-broadcastable output operand with shape (2,) doesn't match the broadcast shape (2,2)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this to get plots\n",
    "run_examples(\n",
    "    optimizer_1=dfp_path,\n",
    "    optimizer_name_1=\"dfp\",\n",
    "    optimizer_2=gradient_descent_path,\n",
    "    optimizer_name_2=\"gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e5872-402c-45c2-b9f0-08aa2cbcb718",
   "metadata": {},
   "source": [
    "## Task 2: BFGS method\n",
    "\n",
    "Implement the BFGS method using the above provided backtracking line search.\n",
    "* `x0` is the initial point.\n",
    "* `f` is the function you are trying to minimize.\n",
    "* `g` is the gradient of `f`.\n",
    "\n",
    "Function `bfgs_path` should return a list of vectors on the path to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826621a3-fb82-4d1c-949f-509d2a7abc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs_path(\n",
    "    x0: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    max_iter: int = 100,\n",
    ") -> list[NDArray]:\n",
    "    \"\"\"Performs the BFGS (Broyden-Fletcher-Goldfarb-Shanno) quasi-Newton method.\n",
    "\n",
    "    Args:\n",
    "        x0 (NDArray): The starting point.\n",
    "        f (Callable[[NDArray], float]): The objective function.\n",
    "        g (Callable[[NDArray], NDArray]): The gradient of the objective function.\n",
    "        max_iter (int, optional): The maximum number of steps. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        list[NDArray]: The list of points visited during the optimization.\n",
    "    \"\"\"\n",
    "    # TODO: Implement BFGS quasi-Newton method using the provided backtracking line search\n",
    "\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aefadc4-6bc2-4837-99e4-086085fe0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to get plots\n",
    "run_examples(\n",
    "    optimizer_1=bfgs_path,\n",
    "    optimizer_name_1=\"bfgs\",\n",
    "    optimizer_2=gradient_descent_path,\n",
    "    optimizer_name_2=\"gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a9ce0-1a09-45e0-953e-6836a52c8474",
   "metadata": {},
   "source": [
    "## Task 3: Error plots\n",
    "\n",
    "Then compare and plot the error over time for the optimization methods on the following tasks.\n",
    "Implementations of the prior methods are given in the Utils section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(\n",
    "    x0: NDArray,\n",
    "    f: Callable[[NDArray], float],\n",
    "    g: Callable[[NDArray], NDArray],\n",
    "    h: Callable[[NDArray], NDArray],\n",
    "    x_star: NDArray,\n",
    ") -> None:\n",
    "    \"\"\"Compares the difference between optimum and function value along the paths of gradient descent, heavy ball and Nesterov's method.\n",
    "\n",
    "    Args:\n",
    "        x0 (NDArray): The starting point.\n",
    "        f (Callable[[NDArray], float]): The objective function.\n",
    "        g (Callable[[NDArray], NDArray]): The gradient of the objective function.\n",
    "        x_star (NDArray): The optimal solution.\n",
    "    \"\"\"\n",
    "    ball = heavy_ball_path(x0, f, g)\n",
    "    nest = nag_path(x0, f, g)\n",
    "    gd = gradient_descent_path(x0, f, g)\n",
    "    newt = newtons_path(x0, f, g, h)\n",
    "    dfp = dfp_path(x0, f, g)\n",
    "    bfgs = bfgs_path(x0, f, g)\n",
    "\n",
    "    def errors(path: list[NDArray]) -> NDArray:\n",
    "        return np.array([f(x) for x in path]) - f(x_star)\n",
    "\n",
    "    plt.semilogy(\n",
    "        np.arange(len(gd)),\n",
    "        errors(gd),\n",
    "        label=\"gradient descent\",\n",
    "    )\n",
    "    plt.semilogy(\n",
    "        np.arange(len(ball)),\n",
    "        errors(ball),\n",
    "        label=\"heavy ball\",\n",
    "    )\n",
    "    plt.semilogy(\n",
    "        np.arange(len(nest)),\n",
    "        errors(nest),\n",
    "        label=\"nesterov\",\n",
    "    )\n",
    "    plt.semilogy(\n",
    "        np.arange(len(newt)),\n",
    "        errors(newt),\n",
    "        label=\"newton's\",\n",
    "    )\n",
    "    plt.semilogy(\n",
    "        np.arange(len(dfp)),\n",
    "        errors(dfp),\n",
    "        label=\"dfp\",\n",
    "    )\n",
    "    plt.semilogy(\n",
    "        np.arange(len(bfgs)),\n",
    "        errors(bfgs),\n",
    "        label=\"bfgs\",\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b032adb-e1c2-4d0f-a258-d0ba56ee3e28",
   "metadata": {},
   "source": [
    "### Simple quadratic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d75b4-4dca-41eb-b2ac-55144aa0126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0] ** 2 + 30 * x[1] ** 2 + 4 * x[0]\n",
    "\n",
    "\n",
    "def g(x):\n",
    "    return np.array([2 * x[0] + 4, 60 * x[1]])\n",
    "\n",
    "\n",
    "def h(x):\n",
    "    return np.array([[2, 0], [0, 60]])\n",
    "\n",
    "\n",
    "x0 = np.array([2.0, 3.0])\n",
    "x_star = np.array([-2.0, 0.0])\n",
    "\n",
    "plot_error(x0, f, g, h, x_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883cf42",
   "metadata": {},
   "source": [
    "### Ill-conditioned quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "rng = np.random.default_rng(seed=42)\n",
    "Q = rng.standard_normal((n, n))\n",
    "Q = Q.T @ Q\n",
    "U, _ = np.linalg.qr(Q)\n",
    "\n",
    "A = U @ np.diag(np.linspace(1, 1000, n)) @ U.T\n",
    "b = rng.standard_normal(n)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 0.5 * x.T @ A @ x - b.T @ x\n",
    "\n",
    "\n",
    "def g(x):\n",
    "    return A @ x - b\n",
    "\n",
    "\n",
    "def h(x):\n",
    "    return A\n",
    "\n",
    "\n",
    "x0 = np.zeros(n)\n",
    "x_star = np.linalg.solve(A, b)\n",
    "\n",
    "plot_error(x0, f, g, h, x_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510aa632-0b89-449f-a81b-5d38eec3e2be",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9e1b6-a8e4-42f8-80c2-2661b099f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    return np.linalg.norm(X @ w - y) ** 2 / len(X)\n",
    "\n",
    "\n",
    "def g(w):\n",
    "    return 2 * X.T @ (X @ w - y) / len(X)\n",
    "\n",
    "\n",
    "def h(w):\n",
    "    return 2 * X.T @ X / len(X)\n",
    "\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=40, random_state=0)\n",
    "x0 = np.zeros(100)\n",
    "\n",
    "x_star = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "plot_error(x0, f, g, h, x_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104abd0-5728-4b25-ada7-3c574379f339",
   "metadata": {},
   "source": [
    "### Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacc5fb-e053-4704-9634-ffe60ee42186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    return np.log(1.0 + np.exp(-y * X.dot(w))).mean() + np.linalg.norm(w) ** 2\n",
    "\n",
    "\n",
    "def g(w):\n",
    "    sig = np.exp(-y * X.dot(w))\n",
    "    return 2 * w - X.T.dot(sig * y / (sig + 1.0)) / X.shape[0]\n",
    "\n",
    "\n",
    "def h(w):\n",
    "    X_rows, w_rows = X.shape\n",
    "    t_0 = np.exp(-(y * (X).dot(w)))\n",
    "    t_1 = t_0 * y\n",
    "    t_2 = np.ones(X_rows) + t_0\n",
    "    hess = (2 * np.eye(w_rows, w_rows)) - 1 / X_rows * (\n",
    "        (((((t_1 * t_0) * y) / (t_2 * t_2))[:, np.newaxis] * X).T).dot(X)\n",
    "        - (((((y * t_0) * y) / t_2)[:, np.newaxis] * X).T).dot(X)\n",
    "    )\n",
    "    return hess\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "    1000,\n",
    "    80,\n",
    "    n_informative=40,\n",
    "    # n_redundant=0,\n",
    "    n_clusters_per_class=2,\n",
    "    flip_y=0.1,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "x0 = np.zeros(80)\n",
    "x_star = minimize(f, x0, jac=g).x\n",
    "\n",
    "plot_error(x0, f, g, h, x_star)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
